{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "37552a1b-dfd8-4d0e-961c-6e8946e6f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords  # import the stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "870a6c44-d0e0-4fe1-9696-06a80c98a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        \n",
    "        self.comment_list = []\n",
    "\n",
    "        self.word_to_index = {} # word to unique-id\n",
    "        self.index_to_word = {} # unique-id to word\n",
    "\n",
    "        # How many times each word occurs in our data after filtering\n",
    "        self.word_counts = Counter()\n",
    "        self.word_embedd = np.array  #300 is the embedd dimension which is fixed here\n",
    "        self.result = []\n",
    "        self.data_train = []\n",
    "        self.x_input = []\n",
    "\n",
    "    def load_data(self, data_arr):        \n",
    "\n",
    "        # 1: Use jieba to do word segmentation\n",
    "        for n in range(len(data_arr)):\n",
    "            words = [w for w in jieba.lcut(data_arr[n]) if w.isalpha]\n",
    "            self.comment_list.append(words)\n",
    "            \n",
    "        # Jieba not only do sentence segmentation, but also tokennize\n",
    "        total_words = []\n",
    "        for w in self.comment_list:\n",
    "            total_words = total_words + w  # load the whole corpus\n",
    "            \n",
    "        self.word_counts = Counter(total_words)\n",
    "            \n",
    "        # 2: Creat word to id mapping\n",
    "        word_list = list(self.word_counts.keys())\n",
    "        for i in range(len(word_list)):\n",
    "            self.word_to_index[word_list[i]] = i\n",
    "            self.index_to_word[i] = word_list[i]\n",
    "        \n",
    "        self.word_to_index.update({'<UNK>': len(self.word_to_index), '<PAD>': len(self.word_to_index)+1})\n",
    "        self.index_to_word.update({len(self.word_to_index): '<UNK>', len(self.word_to_index)+1 :'<PAD>'})\n",
    "            \n",
    "    def load_pre_trained_embedding(self, file_path, dimmension):\n",
    "        self.word_embedd = np.random.rand(len(self.word_to_index), dimmension)  # 部分是随机数，可以用0去赋值来查看那些词没有embedding\n",
    "        \n",
    "        f = open(file_path, \"r\", encoding='UTF-8')\n",
    "        for i, vec in enumerate(f.readlines()):\n",
    "            vector = vec.strip().split(\" \")\n",
    "            if vector[0] in self.word_to_index:\n",
    "                index = self.word_to_index[vector[0]]\n",
    "                emb = [float(x) for x in vector[1:301]]  # extract the pretrained embedding\n",
    "                self.word_embedd[index] = np.asarray(emb, dtype='float32')                \n",
    "        f.close()\n",
    "        np.savez_compressed('pretrained_embedd', embeddings=self.word_embedd)\n",
    "                \n",
    "    \n",
    "    def gen_dataset(self, label_arr, pad_size=15):\n",
    "\n",
    "        for c in range(len(self.comment_list)):\n",
    "            \n",
    "            comment = self.comment_list[c]\n",
    "            comment_id = []\n",
    "            if len(comment) < pad_size:\n",
    "                comment.extend(['<PAD>'] * (pad_size - len(comment)))\n",
    "            else:\n",
    "                comment = comment[:pad_size]\n",
    "            # convert word to id    \n",
    "            for w in comment:\n",
    "                comment_id.append(self.word_to_index[w])\n",
    "            # the format is [([1,2,3],2), ([2,3,4],0),,,,]    \n",
    "            self.result.append((np.array(comment_id), np.array(int(label_arr[c]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9f5fccb6-f949-4eb9-828e-493128bdec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = np.load(\"clf_train_2.npy\", allow_pickle=True)\n",
    "label_pre =  data_pre[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "65d0468a-3ed2-4619-9ba6-9df0f4f8267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "corpus.load_data(data_pre[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "068f1352-8bb0-4c95-8306-8a5c7e6c4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.load_pre_trained_embedding('data/sgns.sogou.char', 300)\n",
    "corpus.gen_dataset(label_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "68b80a4f-d73b-4c8e-b0df-ec04081d059a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "whole_data = corpus.result\n",
    "random.shuffle(whole_data)\n",
    "train_data = whole_data[:20000]\n",
    "test_data = whole_data[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "57f34a9d-fc4a-4157-b737-12d04b4e5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.DataLoader(dataset=train_data,    # load the data\n",
    "                                           batch_size=50, \n",
    "                                           shuffle=True)\n",
    "test = torch.utils.data.DataLoader(dataset=test_data,    # load the data\n",
    "                                           batch_size=50, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "be597340-4407-4a5e-a6ae-f7764741c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dimen, sentence_len, num_filters, dropout):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.conv3 = nn.Conv2d(1, num_filters, (3, embedding_dimen))\n",
    "        self.conv4 = nn.Conv2d(1, num_filters, (4, embedding_dimen))\n",
    "        self.conv5 = nn.Conv2d(1, num_filters, (5, embedding_dimen))\n",
    "        self.Max3_pool = nn.MaxPool2d((sentence_len-3+1, 1))\n",
    "        self.Max4_pool = nn.MaxPool2d((sentence_len-4+1, 1))\n",
    "        self.Max5_pool = nn.MaxPool2d((sentence_len-5+1, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(3*num_filters, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        # Convolution\n",
    "        x = x.unsqueeze(1)\n",
    "        x1 = F.relu(self.conv3(x))\n",
    "        x2 = F.relu(self.conv4(x))\n",
    "        x3 = F.relu(self.conv5(x))\n",
    "\n",
    "        # Pooling\n",
    "        x1 = self.Max3_pool(x1)\n",
    "        x2 = self.Max4_pool(x2)\n",
    "        x3 = self.Max5_pool(x3)\n",
    "\n",
    "        # capture and concatenate the features\n",
    "        x = torch.cat((x1, x2, x3), -1)\n",
    "        x = x.view(batch, 1, -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # project the features to the labels\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 2)  # 2 is the number of the label\n",
    "        # print(x.shape)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b5f768d0-01bd-47d5-9563-e1a121b97f91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a709123f171146fc8f725325157910b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2  The loss is: 0.33983\n",
      "epoch 4  The loss is: 0.30825\n",
      "epoch 7  The loss is: 0.29362\n",
      "epoch 9  The loss is: 0.28416\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epoch = 10\n",
    "\n",
    "model = TextCNN(300, 15, 10, 0.5)  # embedding, sentence len, num_filter, dropout\n",
    "weight = torch.FloatTensor(corpus.word_embedd)\n",
    "embeds = nn.Embedding.from_pretrained(weight)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer = SummaryWriter()  # tensorboard summary writer\n",
    "\n",
    "count = 0\n",
    "loss_sum = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    for data, label in train:\n",
    "        \n",
    "        input_data = embeds(data)\n",
    "        out = model(input_data)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        loss_sum += loss\n",
    "        count += 1\n",
    "\n",
    "        if count % 1000 == 0:\n",
    "            print(\"epoch\", epoch, end='  ')\n",
    "            print(\"The loss is: %.5f\" % (loss_sum/1000))\n",
    "            writer.add_scalar(\"Loss/train\", loss_sum/10000, count+1)\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67224b9a-e585-4589-a6d9-bf1f4041ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(corpus.word_embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14e8b2d7-540f-47b1-a035-ff784466e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = nn.Embedding.from_pretrained(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f061b2-56f8-4459-8308-ef89afc070b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for texts, labels in train:\n",
    "        if total == 0:\n",
    "            print(texts.shape)\n",
    "        outputs = model(embeds(texts))\n",
    "        _, predicted = torch.max(outputs.data, 1)  # the location of the max outputs\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        TP += ((predicted == 1) & (labels.data == 1)).sum()\n",
    "        TN += ((predicted == 0) & (labels.data == 0)).sum()\n",
    "        FN += ((predicted == 0) & (labels.data == 1)).sum()\n",
    "        FP += ((predicted == 1) & (labels.data == 0)).sum()\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * recall * precision / (recall + precision)\n",
    "    print('Accuracy: {} %'.format(100 * correct / total))\n",
    "    print('Precision: {} %'.format(100 * precision))\n",
    "    print('Recall: {} %'.format(100 * recall))\n",
    "    print('F1 Score: {} %'.format(100 * F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22150be0-575c-488d-901f-5908d86cc0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
